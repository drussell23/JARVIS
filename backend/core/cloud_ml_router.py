"""
Cloud ML Router for Voice Biometric Authentication
===================================================

Routes ML operations (speaker verification, embedding extraction) to either
local processing or GCP cloud based on the startup decision.

v17.8.7 Architecture:
- If RAM >= 6GB at startup â†’ LOCAL_FULL â†’ Process locally (instant)
- If RAM < 6GB at startup  â†’ CLOUD_FIRST â†’ Route to GCP Spot VM (instant, no local RAM spike)
- If RAM < 2GB at startup  â†’ CLOUD_ONLY â†’ All ML on GCP (emergency mode)

This eliminates the "Processing..." hang that occurred when LOCAL_MINIMAL mode
deferred model loading until voice unlock request.

Features:
- Async throughout for non-blocking operations
- Automatic failover between cloud and local
- Request caching with Helicone integration
- Health monitoring and circuit breaker
- Cost tracking per request
- Zero hardcoding - all config from environment/startup decision
"""

import asyncio
import aiohttp
import logging
import os
import time
import hashlib
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime

logger = logging.getLogger(__name__)


class MLBackend(Enum):
    """ML processing backend"""
    LOCAL = "local"
    GCP_CLOUD = "gcp_cloud"
    HYBRID = "hybrid"  # Local for light ops, cloud for heavy


class MLOperation(Enum):
    """Types of ML operations"""
    SPEAKER_VERIFICATION = "speaker_verification"
    EMBEDDING_EXTRACTION = "embedding_extraction"
    VOICE_ACTIVITY_DETECTION = "voice_activity_detection"
    WHISPER_TRANSCRIPTION = "whisper_transcription"
    ANTI_SPOOFING = "anti_spoofing"
    BEHAVIORAL_ANALYSIS = "behavioral_analysis"


@dataclass
class MLRequest:
    """Request for ML processing"""
    operation: MLOperation
    audio_data: bytes
    speaker_name: Optional[str] = None
    sample_rate: int = 16000
    metadata: Dict[str, Any] = field(default_factory=dict)
    request_id: Optional[str] = None

    def __post_init__(self):
        if not self.request_id:
            # Generate unique request ID
            content_hash = hashlib.md5(self.audio_data[:1000]).hexdigest()[:8]
            self.request_id = f"{self.operation.value}_{int(time.time()*1000)}_{content_hash}"


@dataclass
class MLResponse:
    """Response from ML processing"""
    success: bool
    backend_used: MLBackend
    operation: MLOperation
    result: Dict[str, Any]
    processing_time_ms: float
    cost_usd: float = 0.0
    cached: bool = False
    request_id: Optional[str] = None
    error: Optional[str] = None


@dataclass
class CircuitBreakerState:
    """Circuit breaker for backend health"""
    failures: int = 0
    last_failure_time: float = 0
    is_open: bool = False
    half_open_until: float = 0

    # Thresholds
    failure_threshold: int = 3
    recovery_timeout: float = 30.0  # seconds


class CloudMLRouter:
    """
    Intelligent ML request router that routes to local or cloud based on
    the startup decision and current system state.

    Prevents "Processing..." hangs by ensuring ML operations never try to
    load heavy models on RAM-constrained systems.
    """

    def __init__(self):
        """Initialize the cloud ML router"""
        # Backend configuration (from environment)
        self.gcp_ml_endpoint = os.getenv("GCP_ML_ENDPOINT", "")
        self.gcp_project = os.getenv("GCP_PROJECT_ID", "jarvis-473803")

        # Current routing mode (set by startup decision)
        self._current_backend = MLBackend.LOCAL
        self._startup_decision = None
        self._gcp_vm_ip: Optional[str] = None

        # Circuit breakers for each backend
        self._circuit_breakers: Dict[MLBackend, CircuitBreakerState] = {
            MLBackend.LOCAL: CircuitBreakerState(),
            MLBackend.GCP_CLOUD: CircuitBreakerState(),
        }

        # Request caching (Helicone-style)
        self._cache: Dict[str, MLResponse] = {}
        self._cache_ttl = 300  # 5 minutes
        self._cache_enabled = True

        # Performance tracking
        self._stats = {
            "total_requests": 0,
            "local_requests": 0,
            "cloud_requests": 0,
            "cache_hits": 0,
            "total_cost_usd": 0.0,
            "avg_local_latency_ms": 0.0,
            "avg_cloud_latency_ms": 0.0,
        }

        # HTTP session for cloud requests
        self._http_session: Optional[aiohttp.ClientSession] = None

        # Local ML components (lazy loaded)
        self._local_speaker_service = None
        self._local_whisper = None

        logger.info("CloudMLRouter initialized (v17.8.7 - No LOCAL_MINIMAL gap)")

    async def initialize(self, startup_decision: Optional['StartupDecision'] = None):
        """
        Initialize the router based on startup decision.

        Args:
            startup_decision: Decision from MemoryAwareStartup
        """
        self._startup_decision = startup_decision

        if startup_decision:
            if startup_decision.use_cloud_ml:
                self._current_backend = MLBackend.GCP_CLOUD
                logger.info(f"â˜ï¸  CloudMLRouter: Routing to GCP (reason: {startup_decision.reason})")

                # Get GCP VM endpoint if available
                if startup_decision.gcp_vm_required:
                    await self._setup_gcp_endpoint()
            else:
                self._current_backend = MLBackend.LOCAL
                logger.info(f"ðŸ’» CloudMLRouter: Routing locally (reason: {startup_decision.reason})")

                # Pre-load local services
                await self._setup_local_services()
        else:
            # Default to local if no startup decision
            self._current_backend = MLBackend.LOCAL
            logger.info("ðŸ’» CloudMLRouter: Default to local (no startup decision)")

        # Create HTTP session for cloud requests
        if self._current_backend == MLBackend.GCP_CLOUD:
            self._http_session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30)
            )

        logger.info(f"âœ… CloudMLRouter ready (backend: {self._current_backend.value})")

    async def _setup_gcp_endpoint(self):
        """Setup GCP ML endpoint from VM manager"""
        try:
            from core.memory_aware_startup import get_startup_manager

            manager = await get_startup_manager()
            if manager._gcp_vm_manager and manager._gcp_vm_manager.managed_vms:
                # Get first healthy VM
                for vm in manager._gcp_vm_manager.managed_vms.values():
                    if vm.is_healthy and vm.ip_address:
                        self._gcp_vm_ip = vm.ip_address
                        self.gcp_ml_endpoint = f"http://{vm.ip_address}:8010/api/ml"
                        logger.info(f"â˜ï¸  GCP ML endpoint: {self.gcp_ml_endpoint}")
                        break

            if not self.gcp_ml_endpoint:
                logger.warning("âš ï¸  No GCP VM available, will create on first request")

        except Exception as e:
            logger.error(f"Failed to setup GCP endpoint: {e}")

    async def _setup_local_services(self):
        """Pre-load local ML services for instant response"""
        try:
            # Import speaker service
            from voice.speaker_verification_service import get_speaker_service
            self._local_speaker_service = get_speaker_service()
            logger.info("âœ… Local speaker service loaded")

        except ImportError as e:
            logger.warning(f"Local speaker service not available: {e}")
        except Exception as e:
            logger.error(f"Failed to load local services: {e}")

    async def route_request(self, request: MLRequest) -> MLResponse:
        """
        Route an ML request to the appropriate backend.

        This is the main entry point for all ML operations.
        Automatically handles:
        - Backend selection based on startup decision
        - Caching for repeated requests
        - Circuit breaker for fault tolerance
        - Failover between backends

        Args:
            request: The ML request to process

        Returns:
            MLResponse with results and metadata
        """
        start_time = time.time()
        self._stats["total_requests"] += 1

        # Check cache first
        cache_key = self._get_cache_key(request)
        if self._cache_enabled and cache_key in self._cache:
            cached = self._cache[cache_key]
            if time.time() - cached.processing_time_ms < self._cache_ttl * 1000:
                self._stats["cache_hits"] += 1
                logger.debug(f"Cache hit for {request.operation.value}")
                return MLResponse(
                    success=cached.success,
                    backend_used=cached.backend_used,
                    operation=request.operation,
                    result=cached.result,
                    processing_time_ms=0.1,  # Cache lookup time
                    cost_usd=0.0,  # No cost for cached
                    cached=True,
                    request_id=request.request_id,
                )

        # Determine backend to use
        backend = self._select_backend(request)

        # Process request
        try:
            if backend == MLBackend.GCP_CLOUD:
                response = await self._process_cloud(request)
                self._stats["cloud_requests"] += 1
            else:
                response = await self._process_local(request)
                self._stats["local_requests"] += 1

            # Record success
            self._record_success(backend)

            # Update cache
            if self._cache_enabled and response.success:
                self._cache[cache_key] = response

            # Track cost
            self._stats["total_cost_usd"] += response.cost_usd

            return response

        except Exception as e:
            # Record failure
            self._record_failure(backend)

            # Try failover
            if backend == MLBackend.GCP_CLOUD:
                logger.warning(f"â˜ï¸  Cloud failed, trying local failover: {e}")
                try:
                    response = await self._process_local(request)
                    response.backend_used = MLBackend.LOCAL
                    return response
                except Exception as local_e:
                    logger.error(f"Local failover also failed: {local_e}")

            return MLResponse(
                success=False,
                backend_used=backend,
                operation=request.operation,
                result={},
                processing_time_ms=(time.time() - start_time) * 1000,
                cost_usd=0.0,
                request_id=request.request_id,
                error=str(e),
            )

    def _select_backend(self, request: MLRequest) -> MLBackend:
        """
        Select the best backend for a request.

        Priority:
        1. Use startup decision backend if healthy
        2. Failover to other backend if circuit breaker is open
        """
        preferred = self._current_backend

        # Check circuit breaker
        cb = self._circuit_breakers[preferred]
        if cb.is_open:
            if time.time() > cb.half_open_until:
                # Try half-open
                cb.is_open = False
                logger.info(f"Circuit breaker half-open for {preferred.value}")
            else:
                # Failover to other backend
                preferred = (
                    MLBackend.LOCAL if preferred == MLBackend.GCP_CLOUD
                    else MLBackend.GCP_CLOUD
                )
                logger.warning(f"Circuit breaker open, failing over to {preferred.value}")

        return preferred

    async def _process_local(self, request: MLRequest) -> MLResponse:
        """Process request locally"""
        start_time = time.time()

        if request.operation == MLOperation.SPEAKER_VERIFICATION:
            result = await self._local_speaker_verification(request)
        elif request.operation == MLOperation.EMBEDDING_EXTRACTION:
            result = await self._local_embedding_extraction(request)
        elif request.operation == MLOperation.VOICE_ACTIVITY_DETECTION:
            result = await self._local_vad(request)
        else:
            raise ValueError(f"Unsupported local operation: {request.operation}")

        processing_time = (time.time() - start_time) * 1000

        # Update average latency
        n = self._stats["local_requests"] + 1
        self._stats["avg_local_latency_ms"] = (
            (self._stats["avg_local_latency_ms"] * (n - 1) + processing_time) / n
        )

        return MLResponse(
            success=True,
            backend_used=MLBackend.LOCAL,
            operation=request.operation,
            result=result,
            processing_time_ms=processing_time,
            cost_usd=0.0,  # Local is free
            request_id=request.request_id,
        )

    async def _process_cloud(self, request: MLRequest) -> MLResponse:
        """Process request on GCP cloud"""
        start_time = time.time()

        if not self.gcp_ml_endpoint:
            # Try to get endpoint
            await self._setup_gcp_endpoint()

        if not self.gcp_ml_endpoint:
            # Still no endpoint - create VM on demand
            await self._create_gcp_vm_on_demand()

        if not self.gcp_ml_endpoint:
            raise RuntimeError("No GCP ML endpoint available")

        # Prepare request payload
        import base64
        payload = {
            "operation": request.operation.value,
            "audio_data": base64.b64encode(request.audio_data).decode(),
            "speaker_name": request.speaker_name,
            "sample_rate": request.sample_rate,
            "metadata": request.metadata,
            "request_id": request.request_id,
        }

        # Send to GCP
        async with self._http_session.post(
            f"{self.gcp_ml_endpoint}/{request.operation.value}",
            json=payload,
            headers={"Content-Type": "application/json"}
        ) as resp:
            if resp.status != 200:
                error_text = await resp.text()
                raise RuntimeError(f"GCP ML error: {resp.status} - {error_text}")

            result = await resp.json()

        processing_time = (time.time() - start_time) * 1000

        # Calculate cost (approximate for Spot VM)
        # e2-highmem-4 Spot: ~$0.029/hour = ~$0.000008/second
        cost_per_second = 0.029 / 3600
        cost = (processing_time / 1000) * cost_per_second

        # Update average latency
        n = self._stats["cloud_requests"] + 1
        self._stats["avg_cloud_latency_ms"] = (
            (self._stats["avg_cloud_latency_ms"] * (n - 1) + processing_time) / n
        )

        return MLResponse(
            success=True,
            backend_used=MLBackend.GCP_CLOUD,
            operation=request.operation,
            result=result,
            processing_time_ms=processing_time,
            cost_usd=cost,
            request_id=request.request_id,
        )

    async def _create_gcp_vm_on_demand(self):
        """Create GCP VM on demand if not available"""
        try:
            from core.memory_aware_startup import get_startup_manager

            manager = await get_startup_manager()
            result = await manager.activate_cloud_ml_backend()

            if result.get("success") and result.get("ip"):
                self._gcp_vm_ip = result["ip"]
                self.gcp_ml_endpoint = f"http://{result['ip']}:8010/api/ml"
                logger.info(f"â˜ï¸  Created GCP VM on demand: {self.gcp_ml_endpoint}")

        except Exception as e:
            logger.error(f"Failed to create GCP VM on demand: {e}")

    async def _local_speaker_verification(self, request: MLRequest) -> Dict[str, Any]:
        """Perform local speaker verification"""
        if not self._local_speaker_service:
            from voice.speaker_verification_service import get_speaker_service
            self._local_speaker_service = get_speaker_service()

        result = await self._local_speaker_service.verify_speaker_enhanced(
            request.audio_data,
            speaker_name=request.speaker_name,
            context=request.metadata
        )

        return result

    async def _local_embedding_extraction(self, request: MLRequest) -> Dict[str, Any]:
        """Extract voice embedding locally"""
        if not self._local_speaker_service:
            from voice.speaker_verification_service import get_speaker_service
            self._local_speaker_service = get_speaker_service()

        embedding = await self._local_speaker_service.extract_embedding(
            request.audio_data
        )

        return {"embedding": embedding.tolist() if hasattr(embedding, 'tolist') else embedding}

    async def _local_vad(self, request: MLRequest) -> Dict[str, Any]:
        """Perform local voice activity detection"""
        import numpy as np

        # Simple energy-based VAD
        audio = np.frombuffer(request.audio_data, dtype=np.int16).astype(np.float32)
        audio = audio / 32768.0

        # Calculate RMS energy
        frame_size = int(request.sample_rate * 0.025)  # 25ms frames
        hop_size = int(request.sample_rate * 0.010)    # 10ms hop

        energies = []
        for i in range(0, len(audio) - frame_size, hop_size):
            frame = audio[i:i + frame_size]
            energy = np.sqrt(np.mean(frame ** 2))
            energies.append(energy)

        # Detect speech regions
        threshold = np.mean(energies) * 1.5
        is_speech = [e > threshold for e in energies]
        speech_ratio = sum(is_speech) / len(is_speech) if is_speech else 0

        return {
            "has_speech": speech_ratio > 0.1,
            "speech_ratio": speech_ratio,
            "avg_energy": float(np.mean(energies)) if energies else 0,
        }

    def _get_cache_key(self, request: MLRequest) -> str:
        """Generate cache key for request"""
        # Hash audio content + operation + speaker
        content = f"{request.operation.value}:{request.speaker_name}:"
        content += hashlib.md5(request.audio_data).hexdigest()
        return hashlib.sha256(content.encode()).hexdigest()[:32]

    def _record_success(self, backend: MLBackend):
        """Record successful request"""
        cb = self._circuit_breakers[backend]
        cb.failures = 0
        cb.is_open = False

    def _record_failure(self, backend: MLBackend):
        """Record failed request and update circuit breaker"""
        cb = self._circuit_breakers[backend]
        cb.failures += 1
        cb.last_failure_time = time.time()

        if cb.failures >= cb.failure_threshold:
            cb.is_open = True
            cb.half_open_until = time.time() + cb.recovery_timeout
            logger.warning(f"Circuit breaker OPEN for {backend.value}")

    def get_stats(self) -> Dict[str, Any]:
        """Get router statistics"""
        return {
            **self._stats,
            "current_backend": self._current_backend.value,
            "gcp_endpoint": self.gcp_ml_endpoint or "not configured",
            "cache_size": len(self._cache),
            "circuit_breakers": {
                backend.value: {
                    "is_open": cb.is_open,
                    "failures": cb.failures,
                }
                for backend, cb in self._circuit_breakers.items()
            }
        }

    async def cleanup(self):
        """Cleanup resources"""
        if self._http_session:
            await self._http_session.close()

        self._cache.clear()
        logger.info("CloudMLRouter cleaned up")


# ============================================================================
# Convenience Functions
# ============================================================================

_router_instance: Optional[CloudMLRouter] = None


async def get_cloud_ml_router() -> CloudMLRouter:
    """Get or create the global cloud ML router"""
    global _router_instance

    if _router_instance is None:
        _router_instance = CloudMLRouter()

        # Initialize with current startup decision
        try:
            from core.memory_aware_startup import get_startup_manager
            manager = await get_startup_manager()
            await _router_instance.initialize(manager.startup_decision)
        except Exception as e:
            logger.warning(f"Could not get startup decision: {e}")
            await _router_instance.initialize(None)

    return _router_instance


async def verify_speaker_cloud_aware(
    audio_data: bytes,
    speaker_name: str,
    **kwargs
) -> Dict[str, Any]:
    """
    Verify speaker with automatic cloud/local routing.

    This is the main entry point for voice biometric authentication
    that automatically routes based on memory-aware startup decision.

    Args:
        audio_data: Raw audio bytes
        speaker_name: Expected speaker name
        **kwargs: Additional metadata

    Returns:
        Verification result with confidence scores
    """
    router = await get_cloud_ml_router()

    request = MLRequest(
        operation=MLOperation.SPEAKER_VERIFICATION,
        audio_data=audio_data,
        speaker_name=speaker_name,
        metadata=kwargs,
    )

    response = await router.route_request(request)

    if response.success:
        result = response.result
        result["backend_used"] = response.backend_used.value
        result["processing_time_ms"] = response.processing_time_ms
        result["cost_usd"] = response.cost_usd
        result["cached"] = response.cached
        return result
    else:
        return {
            "verified": False,
            "confidence": 0.0,
            "error": response.error,
            "backend_used": response.backend_used.value,
        }


async def extract_embedding_cloud_aware(
    audio_data: bytes,
    **kwargs
) -> Dict[str, Any]:
    """
    Extract voice embedding with automatic cloud/local routing.

    Args:
        audio_data: Raw audio bytes
        **kwargs: Additional metadata

    Returns:
        Embedding extraction result
    """
    router = await get_cloud_ml_router()

    request = MLRequest(
        operation=MLOperation.EMBEDDING_EXTRACTION,
        audio_data=audio_data,
        metadata=kwargs,
    )

    response = await router.route_request(request)

    if response.success:
        return response.result
    else:
        return {"error": response.error}
